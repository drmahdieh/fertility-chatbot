import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFaceHub
from langchain.chains import RetrievalQA
import os

# Ø¹Ù†ÙˆØ§Ù† ØµÙØ­Ù‡
st.set_page_config(page_title="Ú†Øªâ€ŒØ¨Ø§Øª Ù†Ø§Ø¨Ø§Ø±ÙˆØ±ÛŒ Ø¨Ø§ LLaMA3", layout="wide")
st.title("ğŸ¤– Ú†Øªâ€ŒØ¨Ø§Øª Ù†Ø§Ø¨Ø§Ø±ÙˆØ±ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ LLaMA3")

# Ù…Ø³ÛŒØ± PDF
pdf_path = "data/infertility_guide.pdf"

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ PDF
@st.cache_data
def load_and_process_pdf(pdf_path):
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(pages)
    return chunks

# Ø³Ø§Ø®Øª Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§
@st.cache_resource
def create_vectorstore(chunks):
    embeddings = HuggingFaceEmbeddings()
    vectorstore = FAISS.from_documents(chunks, embeddings)
    return vectorstore

# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ LLaMA3
def get_llama_model():
    return HuggingFaceHub(
        repo_id="meta-llama/Meta-Llama-3-8B-Instruct",
        model_kwargs={"temperature": 0.5, "max_new_tokens": 512},
        huggingfacehub_api_token=st.secrets["HUGGINGFACEHUB_API_TOKEN"]
    )

# Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ
chunks = load_and_process_pdf(pdf_path)
vectorstore = create_vectorstore(chunks)
llm = get_llama_model()
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever()
)

# Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ
question = st.text_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:")
if question:
    with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´..."):
        answer = qa_chain.run(question)
        st.success("Ù¾Ø§Ø³Ø®:")
        st.write(answer)
